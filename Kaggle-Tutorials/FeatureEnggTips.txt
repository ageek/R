#####################homesite quote conversion
https://www.kaggle.com/c/homesite-quote-conversion/forums/t/18837/what-is-your-best-single-model-score-and-how-did-you-achieve-it

My best single model score was: Public = 0.96902, Private = 0.96828

Ensembling some XGB variations of model parameters, seeds, different feature engineering raised that to: Public = 0.96910, Private = 0.96855, ranking 17th and 21st respectively.

Here is a list of all feature engineering that improved the score during the competition. The number of features in the end was ~1200.

Remove ~10 features with zero variance and near zero variance.
Remove ~5 featues containing more than 97% -1s.
Replace NAs in 2 columns with a new constant value (100).
Center, Scale, and Box-Cox transformation on numericals.
Extract date features (day, month, day of week, year, quarter, week of year, day since epoch, etc).
One-hot encoding (aka dummy variables) for categoricals and date features.
Count -1s, NAs, ""s in each row and add the counts as new features.
Use xgb.importance and correlation to target to select most important features for the coming step.
Add features: log(x), sqrt(x), 1/x (where x is in the top N important features).
Add features: x+y, x-y, x*y, x/y (where x and y are in the top N important features, or x and y are highly correlated with each other).
Repeat 9 and 10 to perform up to 4 operations (e.g: x+((1/w)*(y-z))).
In retrospect, if I would do things differently or if I had more time, I would:

Not remove some of the features I removed early on and instead do feature selection in the end. Although some are saying on the forums that feature selection did not help much.
Give more time for model parameter grid search. The set of parameters on that popular public script were doing well, so I was lazy to do grid search until the last week.
Follow a better stratified CV scheme to avoid private LB drop, I was only using a 3-fold xgb.cv
Train, tune and stack different types of models (NN, LR, etc).

=========================================================================================

In our group, 'A Few with No Clue' with final pos. 8th, there were some subgroups (Datacousins in my case). Datacousins managed to get PubL 0.96885 with a heavily brute-force approach. We had some clean-up pending but stopped trying to improve our model because Tian was on fire building models scoring PubL 0.9693

This was Datacousins' approach for our Xgboost CV12 scoring 0.96885:

 num_rounds         20000
 early_stopping       500
 eta                    0.0028
 max_depth              6
 subsample              0.83
 colsample_bytree       0.77
COLS_DIFF ('CoverageField1B', 'PropertyField21B'), ('GeographicField6A', 'GeographicField8A'), ('GeographicField6A', 'GeographicField13A'), ('GeographicField8A', 'GeographicField11A'), ('GeographicField8A', 'GeographicField13A'), ('GeographicField11A', 'GeographicField13A')])

COLS_JOIN All combinations of 2 of Top7 features (by feat.imp) 
(we take top 7 features according to xgb feature importance. We get all possible pairs and create a new feature for each pair that is the sum of those 2 columns)

COLS_MULTIPLY All combinations of 3 of Top4 features (by feat.imp) 
(we take top 4 features according to xgb feature importance. We get all possible groups of three and create a new feature for each Trio that is the multiplication of those 3 columns)

DROP_THESE_COLUMNS ['PersonalField8', 'PersonalField37', 'PersonalField40', 'PersonalField49', 'PersonalField50', 'PersonalField51', 'PersonalField52', 'PersonalField53', 'PersonalField54', 'PersonalField56', 'PersonalField60', 'PersonalField64', 'PersonalField67', 'PersonalField68', 'PersonalField69', 'PersonalField70', 'PropertyField5', 'PropertyField6', 'PropertyField9', 'PropertyField10', 'PropertyField20', 'GeographicField10A', 'GeographicField10B', 'GeographicField22A']

EXTRACT_FROM_DATE ['year', 'month', 'dayofweek', 'yearmonth']

COUNT_BY_ROWS [None, nan, -1] 
(this was mentioned in the forums and proved to add value. All those elements are counted and added into one new feature)

DROP_CONSTANTS = 0 (dropping constants worsened the result) 
DROP_FEATURES_ALL_NA = 0

STANDARDIZATION_OF_X = YES (just the floats)

NA_BINARIZE = YES 
(new feature with 1s or 0s depending whether the original feature has a NA or other value)

MONOTONIC_Transformation = YES (Top10 int/str features by feat.imp) 
(for each feature value we create a new feature that assigns to each value the total count of that value. I.E. if Berlin is 23 times, we assign 23, if Madrid is 12 times we assign 12)

ONE_HOT_ENCODE = YES Top 5 features (int/str)

CLASS_MEAN Top10 features (int/str) 
[for each selected column (top10) we create a new feature whose values are the mean of the class (quote conversion flag) of that respective value]

==================================================springleaf competition
https://www.kaggle.com/c/springleaf-marketing-response/forums/t/17081/solution-sharing/96793#post96793

I would be interested in folks, cross validation approach – I tried three fold CV using the whole data set but only predicting two folds. Even with this I felt I could not isolate effective features very well. 
@Konrad, how did you perform CV ?

A few features/settings I found helped, roughly in order of effectiveness, to improve on xgboost models, 
• Bagging predictions – for each new set of features, predict three times and take the row wise average. 
• Parameter tuning – very similar to raddar’s settings. 
• Add a row wise count of NA’s and row wise count of outliers (those 999…’s). These two columns added a relatively big bump in score. 
• Get the differences in all dates. Month and day did not help but week seemed to help. 
• Removing the validation set, training on all training data; and predicting with different ntree values. Then test which is the best ntree against the leaderboard, by submitting the same model prediction, but different values of ntree. 
• Create a TDM for the two job columns, and then take an SVD of it to reduce to 10 columns each 
• Find the interactions which added most to target correlation. Ie. Check the correlation of each individual column to the target, then try the combination of both columns (using + - * / ) and see if the resulting interaction column increased the correlation. Filter down to approx 50 new columns giving the highest bump in correlation to target. 
• Did not remove any columns except the constant ones 
• There were three age columns (got the difference in these three); there were two profession columns – add if both are workers, or only one.

There are a lot of things I’m sorry I missed – 2 level modelling, recoding categorical values to response rate (I tried it one and got big overfit, but should have persevered) . And the data leak mentioned in another post, sounds great

===========================springleaf
congratulations to all winners!

Here's my solution. I list some key points below.

(1) category features: use likelihood to encode it, the way how you do is important, it's easily leaky. I use a cross validation to do it.

(2) logistic regression: feed it's prediction into xgboost. it's similar to likelihood features.

(3) feature engineering: only location and date work.

(4) tuning parameters: best paramter I got: max_depth=18 colsample_bytree=0.3 min_child_weight=10 subsample=0.8 num_round=9666 eta=0.006

(5) merging different xgboost's model by using different features and paramters.

likelyhood encoding in python====

I use pandas and numpy in python.

 def get_ctr_features(data, test, y, ctr_cols, dctr, num):
        data["target"] = y
        dcols = set(test.columns)
        kf = cross_validation.StratifiedKFold(y, n_folds=4, shuffle=True, random_state=11)
        tr = np.zeros((data.shape[0], len(ctr_cols)))
        for kfold, (itr, icv) in enumerate(kf):
            data_tr = data.iloc[itr]
            data_te = data.iloc[icv]
            for t, col in enumerate(ctr_cols):
                if col not in dcols:
                    continue
                ctr_df = data_tr[[col, "target"]].groupby(col).agg(["count", "sum"])
                ctr_dict = ctr_df.apply(lambda x: calc_ctr(x, num), axis=1).to_dict()
                tr[icv, t] = data_te[col].apply(lambda x: ctr_dict.get(x, dctr))

        te = np.zeros((test.shape[0], len(ctr_cols)))
        for t, col in enumerate(ctr_cols):
            if col not in dcols:
                    continue
            ctr_df = data[[col, "target"]].groupby(col).agg(["count", "sum"])
            ctr_dict = ctr_df.apply(lambda x: calc_ctr(x, num), axis=1).to_dict()
            te[:, t] = test[col].apply(lambda x: ctr_dict.get(x, dctr))
        del data["target"]
        return tr, te
===============================